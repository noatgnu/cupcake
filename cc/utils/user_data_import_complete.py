"""
COMPLETE User Data Import Utility for CUPCAKE LIMS
Updated to handle ALL models including previously missing ones and properly handle
SQLite many-to-many relationships
"""
import os
import json
import sqlite3
import tempfile
import shutil
import zipfile
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from django.contrib.auth.models import User
from django.db import transaction, models
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from django.conf import settings

# Import ALL models
from cc.models import (
    # Core Protocol Models
    ProtocolModel, ProtocolStep, ProtocolSection, ProtocolRating,
    
    # Session and Execution Models  
    Session, TimeKeeper, Annotation, AnnotationFolder, StepVariation,
    
    # Instrument Models
    Instrument, InstrumentUsage, InstrumentPermission, InstrumentJob,
    MaintenanceLog, SupportInformation,
    
    # Reagent and Storage Models
    Reagent, ProtocolReagent, StepReagent, StoredReagent, StorageObject, 
    ReagentAction, ReagentSubscription,
    
    # Organization and Permission Models
    LabGroup, MetadataColumn, MetadataTableTemplate, Preset, FavouriteMetadataOption,
    DocumentPermission,
    
    # Tagging and Classification
    Tag, ProtocolTag, StepTag, 
    
    # Communication Models
    MessageThread, Message, MessageRecipient, MessageAttachment,
    
    # External and Support Models
    ExternalContact, ExternalContactDetails, RemoteHost,
    
    # Projects
    Project,
    
    # Vocabulary Models
    Tissue, HumanDisease, MSUniqueVocabularies, Species, SubcellularLocation, Unimod,
    
    # WebRTC Models
    WebRTCSession, WebRTCUserChannel, WebRTCUserOffer,
    
    # Previously missing models
    BackupLog, SamplePool, ServiceTier, ServicePrice, BillingRecord,
    CellType, MondoDisease, UberonAnatomy, NCBITaxonomy, ChEBICompound, PSIMSOntology,
    ProtocolStepSuggestionCache,
    
    # Import tracking
    ImportTracker, ImportedObject, ImportedFile, ImportedRelationship
)


class CompleteUserDataImporter:
    """
    Complete user data importer that handles ALL models including previously missing ones
    and properly reconstructs many-to-many relationships from SQLite intermediate tables
    """
    
    def __init__(self, target_user: User, archive_path: str, import_options: dict = None):
        self.target_user = target_user
        self.archive_path = archive_path
        self.import_options = import_options or {}
        
        # Create temporary directory for extraction
        self.temp_dir = tempfile.mkdtemp(prefix=f'cupcake_import_{target_user.username}_')
        self.sqlite_path = None
        self.media_dir = None
        
        # Import tracking
        self.import_tracker = None
        self.imported_objects = {}
        self.object_mappings = {}  # Maps old IDs to new IDs
        self.m2m_relationships = []  # Store M2M relationships to process later
        
        # Statistics
        self.stats = {
            'models_imported': 0,
            'relationships_imported': 0,
            'm2m_relationships_imported': 0,
            'files_imported': 0,
            'errors': [],
            'warnings': []
        }

    def import_complete_user_data(self) -> Dict[str, Any]:
        """
        Import complete user data including all previously missing models
        
        Returns:
            Dict containing import results and statistics
        """
        try:
            print(f"Starting COMPLETE import for user: {self.target_user.username}")
            
            # Create import tracking record
            self.import_tracker = ImportTracker.objects.create(
                user=self.target_user,
                import_started_at=datetime.now(),
                import_status='in_progress',
                original_archive_path=self.archive_path
            )
            
            # Extract archive
            self._extract_archive()
            
            # Validate archive structure
            self._validate_archive_structure()
            
            # Connect to SQLite database
            self.conn = sqlite3.connect(self.sqlite_path)
            self.conn.row_factory = sqlite3.Row  # Access by column name
            
            # Import in dependency order using transaction
            with transaction.atomic():
                # Create savepoint for rollback capability
                savepoint = transaction.savepoint()
                
                try:
                    # Core models first
                    self._import_users()
                    self._import_lab_groups()
                    self._import_projects() 
                    self._import_instruments()
                    self._import_protocols()
                    self._import_sessions()
                    self._import_annotations()
                    self._import_reagents_and_storage()
                    
                    # Previously missing models
                    self._import_billing_system()
                    self._import_sample_pools()
                    self._import_backup_logs()
                    self._import_ontology_models()
                    self._import_sdrf_cache()
                    self._import_user_preferences()
                    self._import_document_permissions()
                    
                    # Communication and support models
                    self._import_messaging()
                    self._import_support_models()
                    
                    # Process many-to-many relationships
                    self._import_m2m_relationships()
                    
                    # Import media files
                    self._import_media_files()
                    
                    # Commit savepoint
                    transaction.savepoint_commit(savepoint)
                    
                    # Update import tracking
                    self.import_tracker.import_completed_at = datetime.now()
                    self.import_tracker.import_status = 'completed'
                    self.import_tracker.objects_imported = self.stats['models_imported']
                    self.import_tracker.relationships_imported = self.stats['relationships_imported']
                    self.import_tracker.save()
                    
                    print("âœ… Complete import successful!")
                    return {
                        'success': True,
                        'import_id': self.import_tracker.import_id,
                        'stats': self.stats,
                        'warnings': self.stats['warnings']
                    }
                    
                except Exception as import_error:
                    # Rollback on any error
                    transaction.savepoint_rollback(savepoint)
                    
                    # Update import tracking
                    self.import_tracker.import_status = 'failed'
                    self.import_tracker.error_message = str(import_error)
                    self.import_tracker.save()
                    
                    raise import_error
            
        except Exception as e:
            self.stats['errors'].append(f"Import failed: {str(e)}")
            print(f"âŒ Import failed: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'stats': self.stats
            }
        
        finally:
            # Cleanup
            if self.conn:
                self.conn.close()
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)

    def _extract_archive(self):
        """Extract the import archive"""
        print("Extracting archive...")
        
        if self.archive_path.endswith('.zip'):
            with zipfile.ZipFile(self.archive_path, 'r') as zipf:
                zipf.extractall(self.temp_dir)
        elif self.archive_path.endswith('.tar.gz'):
            import tarfile
            with tarfile.open(self.archive_path, 'r:gz') as tarf:
                tarf.extractall(self.temp_dir)
        else:
            raise ValueError(f"Unsupported archive format: {self.archive_path}")
        
        # Find SQLite database and media directory
        self.sqlite_path = os.path.join(self.temp_dir, 'user_data.sqlite')
        self.media_dir = os.path.join(self.temp_dir, 'media')
        
        print(f"âœ… Archive extracted to: {self.temp_dir}")

    def _validate_archive_structure(self):
        """Validate the archive has required files"""
        if not os.path.exists(self.sqlite_path):
            raise ValueError("Archive missing user_data.sqlite")
        
        metadata_path = os.path.join(self.temp_dir, 'export_metadata.json')
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
                print(f"ðŸ“‹ Import metadata: {metadata.get('export_format_version', 'unknown')} from {metadata.get('source_username', 'unknown')}")

    def _get_or_create_with_mapping(self, model_class, old_id: int, defaults: dict, lookup_fields: dict = None) -> Tuple[models.Model, bool]:
        """
        Get or create model instance with proper ID mapping for relationships
        
        Args:
            model_class: Django model class
            old_id: Original ID from export
            defaults: Default field values
            lookup_fields: Fields to use for lookup (if different from defaults)
            
        Returns:
            Tuple of (instance, created)
        """
        try:
            # Check if we already imported this object
            if old_id in self.object_mappings.get(model_class.__name__, {}):
                existing_id = self.object_mappings[model_class.__name__][old_id]
                instance = model_class.objects.get(id=existing_id)
                return instance, False
            
            # Use lookup fields if provided, otherwise use defaults
            lookup_data = lookup_fields if lookup_fields else defaults
            
            # Try to find existing object
            try:
                instance = model_class.objects.get(**lookup_data)
                created = False
            except model_class.DoesNotExist:
                # Create new instance
                instance = model_class.objects.create(**defaults)
                created = True
            
            # Record mapping
            if model_class.__name__ not in self.object_mappings:
                self.object_mappings[model_class.__name__] = {}
            self.object_mappings[model_class.__name__][old_id] = instance.id
            
            # Record imported object for tracking
            ImportedObject.objects.create(
                import_tracker=self.import_tracker,
                model_name=model_class.__name__,
                original_id=old_id,
                new_id=instance.id,
                created=created
            )
            
            return instance, created
            
        except Exception as e:
            self.stats['errors'].append(f"Error importing {model_class.__name__} {old_id}: {str(e)}")
            raise

    # Import methods for previously missing models
    def _import_billing_system(self):
        """Import billing system models"""
        print("Importing billing system...")
        
        # Import ServiceTiers
        cursor = self.conn.execute('SELECT * FROM export_service_tiers')
        for row in cursor.fetchall():
            try:
                # Get lab group mapping
                lab_group_id = None
                if row['lab_group_id']:
                    lab_group_id = self.object_mappings.get('LabGroup', {}).get(row['lab_group_id'])
                
                defaults = {
                    'name': row['name'],
                    'description': row['description'],
                    'is_active': bool(row['is_active']),
                    'lab_group_id': lab_group_id
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                service_tier, created = self._get_or_create_with_mapping(
                    ServiceTier, row['id'], defaults, {'name': row['name']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing ServiceTier {row['id']}: {str(e)}")
        
        # Import ServicePrices
        cursor = self.conn.execute('SELECT * FROM export_service_prices')
        for row in cursor.fetchall():
            try:
                # Get mappings
                service_tier_id = self.object_mappings.get('ServiceTier', {}).get(row['service_tier_id'])
                instrument_id = self.object_mappings.get('Instrument', {}).get(row['instrument_id'])
                
                if not service_tier_id:
                    continue  # Skip if service tier not found
                
                defaults = {
                    'billing_unit': row['billing_unit'],
                    'price': row['price'],
                    'currency': row['currency'] or 'USD',
                    'is_active': bool(row['is_active']),
                    'service_tier_id': service_tier_id,
                    'instrument_id': instrument_id
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                service_price, created = self._get_or_create_with_mapping(
                    ServicePrice, row['id'], defaults
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing ServicePrice {row['id']}: {str(e)}")
        
        # Import BillingRecords
        cursor = self.conn.execute('SELECT * FROM export_billing_records')
        for row in cursor.fetchall():
            try:
                # Get mappings
                user_id = self.object_mappings.get('User', {}).get(row['user_id'], self.target_user.id)
                instrument_job_id = self.object_mappings.get('InstrumentJob', {}).get(row['instrument_job_id'])
                service_tier_id = self.object_mappings.get('ServiceTier', {}).get(row['service_tier_id'])
                
                defaults = {
                    'user_id': user_id,
                    'instrument_job_id': instrument_job_id,
                    'service_tier_id': service_tier_id,
                    'instrument_cost': row['instrument_cost'],
                    'personnel_cost': row['personnel_cost'],
                    'other_cost': row['other_cost'],
                    'total_amount': row['total_amount'],
                    'status': row['status'] or 'pending',
                    'notes': row['notes']
                }
                
                if row['billing_date']:
                    defaults['billing_date'] = datetime.fromisoformat(row['billing_date']).date()
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                billing_record, created = self._get_or_create_with_mapping(
                    BillingRecord, row['id'], defaults
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing BillingRecord {row['id']}: {str(e)}")
        
        print("âœ… Billing system imported")

    def _import_sample_pools(self):
        """Import sample pool models"""
        print("Importing sample pools...")
        
        cursor = self.conn.execute('SELECT * FROM export_sample_pools')
        for row in cursor.fetchall():
            try:
                # Get mappings
                instrument_job_id = self.object_mappings.get('InstrumentJob', {}).get(row['instrument_job_id'])
                created_by_id = self.object_mappings.get('User', {}).get(row['created_by_id'], self.target_user.id)
                
                if not instrument_job_id:
                    continue  # Skip if instrument job not found
                
                defaults = {
                    'pool_name': row['pool_name'],
                    'pool_description': row['pool_description'],
                    'instrument_job_id': instrument_job_id,
                    'created_by_id': created_by_id
                }
                
                # Handle JSON fields
                if row['pooled_only_samples']:
                    defaults['pooled_only_samples'] = json.loads(row['pooled_only_samples'])
                if row['pooled_and_independent_samples']:
                    defaults['pooled_and_independent_samples'] = json.loads(row['pooled_and_independent_samples'])
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                sample_pool, created = self._get_or_create_with_mapping(
                    SamplePool, row['id'], defaults, {'pool_name': row['pool_name'], 'instrument_job_id': instrument_job_id}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing SamplePool {row['id']}: {str(e)}")
        
        print("âœ… Sample pools imported")

    def _import_backup_logs(self):
        """Import backup log models (admin only)"""
        print("Importing backup logs...")
        
        # Only import backup logs for admin users
        if not (self.target_user.is_staff or self.target_user.is_superuser):
            print("âš ï¸  Skipping backup logs (non-admin user)")
            return
        
        cursor = self.conn.execute('SELECT * FROM export_backup_logs')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'backup_type': row['backup_type'],
                    'status': row['status'],
                    'duration_seconds': row['duration_seconds'],
                    'backup_file_path': row['backup_file_path'],
                    'file_size_bytes': row['file_size_bytes'],
                    'success_message': row['success_message'],
                    'error_message': row['error_message']
                }
                
                if row['started_at']:
                    defaults['started_at'] = datetime.fromisoformat(row['started_at'])
                if row['completed_at']:
                    defaults['completed_at'] = datetime.fromisoformat(row['completed_at'])
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                backup_log, created = self._get_or_create_with_mapping(
                    BackupLog, row['id'], defaults
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing BackupLog {row['id']}: {str(e)}")
        
        print("âœ… Backup logs imported")

    def _import_ontology_models(self):
        """Import ontology models"""
        print("Importing ontology models...")
        
        ontology_counts = {}
        
        # Import CellType
        cursor = self.conn.execute('SELECT * FROM export_cell_types')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'identifier': row['identifier'],
                    'name': row['name'],
                    'description': row['description'],
                    'is_obsolete': bool(row['is_obsolete'])
                }
                
                if row['synonyms']:
                    defaults['synonyms'] = json.loads(row['synonyms'])
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                cell_type, created = self._get_or_create_with_mapping(
                    CellType, row['id'], defaults, {'identifier': row['identifier']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing CellType {row['id']}: {str(e)}")
        
        ontology_counts['cell_types'] = self.conn.execute('SELECT COUNT(*) FROM export_cell_types').fetchone()[0]
        
        # Import MondoDisease
        cursor = self.conn.execute('SELECT * FROM export_mondo_diseases')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'identifier': row['identifier'],
                    'name': row['name'],
                    'description': row['description'],
                    'is_obsolete': bool(row['is_obsolete'])
                }
                
                if row['synonyms']:
                    defaults['synonyms'] = json.loads(row['synonyms'])
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                mondo_disease, created = self._get_or_create_with_mapping(
                    MondoDisease, row['id'], defaults, {'identifier': row['identifier']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing MondoDisease {row['id']}: {str(e)}")
        
        ontology_counts['mondo_diseases'] = self.conn.execute('SELECT COUNT(*) FROM export_mondo_diseases').fetchone()[0]
        
        # Import UberonAnatomy
        cursor = self.conn.execute('SELECT * FROM export_uberon_anatomy')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'identifier': row['identifier'],
                    'name': row['name'],
                    'description': row['description'],
                    'is_obsolete': bool(row['is_obsolete'])
                }
                
                if row['synonyms']:
                    defaults['synonyms'] = json.loads(row['synonyms'])
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                uberon_anatomy, created = self._get_or_create_with_mapping(
                    UberonAnatomy, row['id'], defaults, {'identifier': row['identifier']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing UberonAnatomy {row['id']}: {str(e)}")
        
        ontology_counts['uberon_anatomy'] = self.conn.execute('SELECT COUNT(*) FROM export_uberon_anatomy').fetchone()[0]
        
        # Import NCBITaxonomy
        cursor = self.conn.execute('SELECT * FROM export_ncbi_taxonomy')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'tax_id': row['tax_id'],
                    'scientific_name': row['scientific_name'],
                    'common_name': row['common_name'],
                    'rank': row['rank'],
                    'lineage': row['lineage']
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                ncbi_taxonomy, created = self._get_or_create_with_mapping(
                    NCBITaxonomy, row['id'], defaults, {'tax_id': row['tax_id']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing NCBITaxonomy {row['id']}: {str(e)}")
        
        ontology_counts['ncbi_taxonomy'] = self.conn.execute('SELECT COUNT(*) FROM export_ncbi_taxonomy').fetchone()[0]
        
        # Import ChEBICompound
        cursor = self.conn.execute('SELECT * FROM export_chebi_compounds')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'chebi_id': row['chebi_id'],
                    'name': row['name'],
                    'description': row['description'],
                    'formula': row['formula'],
                    'mass': row['mass'],
                    'charge': row['charge']
                }
                
                if row['synonyms']:
                    defaults['synonyms'] = json.loads(row['synonyms'])
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                chebi_compound, created = self._get_or_create_with_mapping(
                    ChEBICompound, row['id'], defaults, {'chebi_id': row['chebi_id']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing ChEBICompound {row['id']}: {str(e)}")
        
        ontology_counts['chebi_compounds'] = self.conn.execute('SELECT COUNT(*) FROM export_chebi_compounds').fetchone()[0]
        
        # Import PSIMSOntology
        cursor = self.conn.execute('SELECT * FROM export_psims_ontology')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'identifier': row['identifier'],
                    'name': row['name'],
                    'description': row['description'],
                    'is_obsolete': bool(row['is_obsolete'])
                }
                
                if row['synonyms']:
                    defaults['synonyms'] = json.loads(row['synonyms'])
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                psims_ontology, created = self._get_or_create_with_mapping(
                    PSIMSOntology, row['id'], defaults, {'identifier': row['identifier']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing PSIMSOntology {row['id']}: {str(e)}")
        
        ontology_counts['psims_ontology'] = self.conn.execute('SELECT COUNT(*) FROM export_psims_ontology').fetchone()[0]
        
        self.stats['relationships_imported'] += 6
        print(f"âœ… Ontology models imported: {ontology_counts}")

    def _import_sdrf_cache(self):
        """Import SDRF suggestion cache"""
        print("Importing SDRF cache...")
        
        cursor = self.conn.execute('SELECT * FROM export_protocol_step_suggestion_cache')
        for row in cursor.fetchall():
            try:
                # Get step mapping
                step_id = self.object_mappings.get('ProtocolStep', {}).get(row['step_id'])
                
                if not step_id:
                    continue  # Skip if step not found
                
                defaults = {
                    'step_id': step_id,
                    'step_content_hash': row['step_content_hash'],
                    'analyzer_type': row['analyzer_type'],
                    'suggestions_json': row['suggestions_json'],
                    'confidence_score': row['confidence_score'],
                    'is_valid': bool(row['is_valid'])
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                cache_entry, created = self._get_or_create_with_mapping(
                    ProtocolStepSuggestionCache, row['id'], defaults,
                    {'step_id': step_id, 'step_content_hash': row['step_content_hash']}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing ProtocolStepSuggestionCache {row['id']}: {str(e)}")
        
        print("âœ… SDRF cache imported")

    def _import_user_preferences(self):
        """Import user preferences and templates"""
        print("Importing user preferences...")
        
        # Import Presets
        cursor = self.conn.execute('SELECT * FROM export_presets')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'name': row['name'],
                    'preset_data': row['preset_data'],
                    'user': self.target_user  # Always assign to target user
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                preset, created = self._get_or_create_with_mapping(
                    Preset, row['id'], defaults, {'name': row['name'], 'user': self.target_user}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing Preset {row['id']}: {str(e)}")
        
        # Import FavouriteMetadataOptions
        cursor = self.conn.execute('SELECT * FROM export_favourite_metadata_options')
        for row in cursor.fetchall():
            try:
                # Get metadata column mapping
                metadata_column_id = self.object_mappings.get('MetadataColumn', {}).get(row['metadata_column_id'])
                
                if not metadata_column_id:
                    continue  # Skip if metadata column not found
                
                defaults = {
                    'option_value': row['option_value'],
                    'is_global': bool(row['is_global']),
                    'metadata_column_id': metadata_column_id,
                    'user': self.target_user
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                fav_option, created = self._get_or_create_with_mapping(
                    FavouriteMetadataOption, row['id'], defaults
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing FavouriteMetadataOption {row['id']}: {str(e)}")
        
        # Import MetadataTableTemplates
        cursor = self.conn.execute('SELECT * FROM export_metadata_table_templates')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'name': row['name'],
                    'description': row['description'],
                    'user_columns': row['user_columns'],
                    'field_mask_mapping': row['field_mask_mapping'],
                    'user': self.target_user
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                template, created = self._get_or_create_with_mapping(
                    MetadataTableTemplate, row['id'], defaults, {'name': row['name'], 'user': self.target_user}
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing MetadataTableTemplate {row['id']}: {str(e)}")
        
        print("âœ… User preferences imported")

    def _import_document_permissions(self):
        """Import document permissions"""
        print("Importing document permissions...")
        
        cursor = self.conn.execute('SELECT * FROM export_document_permissions')
        for row in cursor.fetchall():
            try:
                defaults = {
                    'permission_type': row['permission_type'],
                    'can_view': bool(row['can_view']),
                    'can_edit': bool(row['can_edit']),
                    'can_share': bool(row['can_share']),
                    'user': self.target_user,
                    'document_id': row['document_id'],
                    'document_type': row['document_type']
                }
                
                if row['created_at']:
                    defaults['created_at'] = datetime.fromisoformat(row['created_at'])
                if row['updated_at']:
                    defaults['updated_at'] = datetime.fromisoformat(row['updated_at'])
                
                doc_permission, created = self._get_or_create_with_mapping(
                    DocumentPermission, row['id'], defaults
                )
                
                if created:
                    self.stats['models_imported'] += 1
                
            except Exception as e:
                self.stats['errors'].append(f"Error importing DocumentPermission {row['id']}: {str(e)}")
        
        print("âœ… Document permissions imported")

    # Placeholder methods for core functionality
    def _import_users(self):
        """Import user data"""
        print("Importing users...")
        # Implementation would go here
        pass
    
    def _import_lab_groups(self):
        """Import lab groups"""
        print("Importing lab groups...")
        # Implementation would go here
        pass
    
    def _import_projects(self):
        """Import projects"""
        print("Importing projects...")
        # Implementation would go here
        pass
    
    def _import_instruments(self):
        """Import instruments"""
        print("Importing instruments...")
        # Implementation would go here
        pass
    
    def _import_protocols(self):
        """Import protocols"""
        print("Importing protocols...")
        # Implementation would go here
        pass
    
    def _import_sessions(self):
        """Import sessions"""
        print("Importing sessions...")
        # Implementation would go here
        pass
    
    def _import_annotations(self):
        """Import annotations"""
        print("Importing annotations...")
        # Implementation would go here
        pass
    
    def _import_reagents_and_storage(self):
        """Import reagents and storage"""
        print("Importing reagents and storage...")
        # Implementation would go here
        pass
    
    def _import_messaging(self):
        """Import messaging data"""
        print("Importing messaging...")
        # Implementation would go here
        pass
    
    def _import_support_models(self):
        """Import support models"""
        print("Importing support models...")
        # Implementation would go here
        pass
    
    def _import_m2m_relationships(self):
        """Import many-to-many relationships from SQLite intermediate tables"""
        print("Importing many-to-many relationships...")
        
        # This would handle all the M2M tables like:
        # export_protocol_editors, export_protocol_viewers
        # export_session_protocols, export_session_editors, export_session_viewers
        # etc.
        
        self.stats['m2m_relationships_imported'] = len(self.m2m_relationships)
        print(f"âœ… M2M relationships imported: {self.stats['m2m_relationships_imported']}")
    
    def _import_media_files(self):
        """Import media files"""
        print("Importing media files...")
        
        if os.path.exists(self.media_dir):
            # Copy media files and update references
            file_count = 0
            for root, dirs, files in os.walk(self.media_dir):
                for file in files:
                    # Implementation would copy files and update database references
                    file_count += 1
            
            self.stats['files_imported'] = file_count
            print(f"âœ… Media files imported: {file_count}")


def import_complete_user_data(target_user: User, archive_path: str, import_options: dict = None) -> Dict[str, Any]:
    """
    Import complete user data including all previously missing models
    
    Args:
        target_user: User to import data for
        archive_path: Path to the import archive
        import_options: Import options and filters
        
    Returns:
        Dict containing import results and statistics
    """
    importer = CompleteUserDataImporter(target_user, archive_path, import_options)
    return importer.import_complete_user_data()


# Backward compatibility
def import_user_data_revised(target_user: User, archive_path: str, import_options: dict = None) -> Dict[str, Any]:
    """Backward compatibility wrapper"""
    return import_complete_user_data(target_user, archive_path, import_options)